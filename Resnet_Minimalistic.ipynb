{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Before ResNet, deep networks suffered from:\n",
        "\n",
        "Vanishing gradients: gradients shrink as they flow backward through layers.\n",
        "\n",
        "Degradation: accuracy gets worse as layers increase ‚Äî deeper isn‚Äôt always better.\n",
        "\n",
        "ResNet introduced a simple idea:\n",
        "\n",
        "Instead of learning a full transformation,\n",
        "just learn the difference from the input (i.e., the residual)."
      ],
      "metadata": {
        "id": "APr7oE7PXfPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in plain nn : out = F (x)\n",
        "\n",
        "but in resnet its out = F(x) + x\n",
        "\n",
        "This lets the network focus on just learning the residual (how the output differs from the input)."
      ],
      "metadata": {
        "id": "-Bs6kWI7X7YA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in case of vanishing gradients now gradients can flow directly to earlier layers\n",
        "\n",
        "Network can easily learn identity mapping if needed (just set residual weights ‚âà 0\n",
        "You can keep stacking layers.\n",
        "\n",
        "If they‚Äôre not useful, the model just skips them by making\n",
        "ùêπ\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "=\n",
        "0\n"
      ],
      "metadata": {
        "id": "avGp9kpDY1ri"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JxCJJiNYXblw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n"
      ]
    }
  ]
}