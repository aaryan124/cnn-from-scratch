{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Before ResNet, deep networks suffered from:\n",
        "\n",
        "Vanishing gradients: gradients shrink as they flow backward through layers.\n",
        "\n",
        "Degradation: accuracy gets worse as layers increase ‚Äî deeper isn‚Äôt always better.\n",
        "\n",
        "ResNet introduced a simple idea:\n",
        "\n",
        "Instead of learning a full transformation,\n",
        "just learn the difference from the input (i.e., the residual)."
      ],
      "metadata": {
        "id": "APr7oE7PXfPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in plain nn : out = F (x)\n",
        "\n",
        "but in resnet its out = F(x) + x\n",
        "\n",
        "This lets the network focus on just learning the residual (how the output differs from the input)."
      ],
      "metadata": {
        "id": "-Bs6kWI7X7YA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in case of vanishing gradients now gradients can flow directly to earlier layers\n",
        "\n",
        "Network can easily learn identity mapping if needed (just set residual weights ‚âà 0\n",
        "You can keep stacking layers.\n",
        "\n",
        "If they‚Äôre not useful, the model just skips them by making\n",
        "ùêπ\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "=\n",
        "0\n"
      ],
      "metadata": {
        "id": "avGp9kpDY1ri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case of shape mismatch we use a shortcut that converts x into the same shape as F(x) using a 1√ó1 Conv"
      ],
      "metadata": {
        "id": "tlohB2E9hGWP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JxCJJiNYXblw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n"
      ]
    }
  ]
}