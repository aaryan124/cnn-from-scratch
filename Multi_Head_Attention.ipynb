{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "🧠 What is Multi-Head vs Single-Head?  \n",
        "\n",
        "Single-Head\tOne set of weight matrices (W_q, W_k, W_v) is used for all attention.  \n",
        "\n",
        "Multi-Head\tMultiple independent sets of (W_q, W_k, W_v), then results are concatenated. Each “head” learns to focus on different patterns."
      ],
      "metadata": {
        "id": "2gfJJ6BFHk0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you apply attention over the whole space using a single projection, then all information must be compressed into a single set of attention scores.\n",
        "\n",
        "Each subspace gets its own Q, K, V projection.\n",
        "\n",
        "Each head independently attends over the sequence using only its subspace.\n",
        "\n",
        "You combine the outputs (via concat + linear layer) to get richer representations."
      ],
      "metadata": {
        "id": "NBX-xF0uRGlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heads dont interact to each other during their attention computation\n",
        "\n",
        "but during softmax on scores we concat all heads output and in final project this using W_o to final output here they all combine"
      ],
      "metadata": {
        "id": "kDCKbc_UShAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each head uses smaller matrices,\n",
        "\n",
        "All heads are computed in parallel (very GPU-friendly)."
      ],
      "metadata": {
        "id": "gUDeSUr4ThmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In CNNs:\n",
        "\n",
        "Receptive field grows slowly — token sees neighbors only after many layers.\n",
        "\n",
        "In attention:\n",
        "\n",
        "Every token attends to all tokens in the same layer.\n",
        "\n",
        "This gives a global receptive field in a single step.\n",
        "\n",
        "Token at position 1 can immediately focus on position 99 if needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "0Xs29-9UYeP0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aCVpd5bzHEeu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x, axis=-1):\n",
        "    exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=axis, keepdims=True)\n",
        "\n",
        "def dsoftmax(softmax_out, grad_out):\n",
        "    s = softmax_out\n",
        "    return s * (grad_out - np.sum(grad_out * s, axis=-1, keepdims=True))\n",
        "\n",
        "class MultiHeadSelfAttention:\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\" # Each head gets a portion of total embedding\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads #Each head works on a subspace of dimension head_dim.\n",
        "\n",
        "        # Parameters\n",
        "        self.W_q = np.random.randn(embed_dim, embed_dim) * 0.1  # Exactly Same as Single Attention\n",
        "        self.W_k = np.random.randn(embed_dim, embed_dim) * 0.1\n",
        "        self.W_v = np.random.randn(embed_dim, embed_dim) * 0.1\n",
        "        self.W_o = np.random.randn(embed_dim, embed_dim) * 0.1\n",
        "\n",
        "        # Gradients\n",
        "        self.dW_q = np.zeros_like(self.W_q)    # Also Same as Single Attention\n",
        "        self.dW_k = np.zeros_like(self.W_k)\n",
        "        self.dW_v = np.zeros_like(self.W_v)\n",
        "        self.dW_o = np.zeros_like(self.W_o)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        T, D = x.shape # T = seq_len ,D = embed_dim\n",
        "\n",
        "        self.Q = x @ self.W_q  # Same as Single Attention\n",
        "        self.K = x @ self.W_k\n",
        "        self.V = x @ self.W_v\n",
        "\n",
        "        self.Q_ = self.Q.reshape(T, self.num_heads, self.head_dim) # shape (T,h,d) where h is num_heads and d : head dim .T : Target seq len or no of queries\n",
        "        self.K_ = self.K.reshape(T, self.num_heads, self.head_dim) # shape (s,h,d) where s is source seq length (no of keys)\n",
        "        self.V_ = self.V.reshape(T, self.num_heads, self.head_dim) # value vector for each source token per head\n",
        "\n",
        "        # How similar keys are to token queries\n",
        "        self.scores = np.einsum('thd,shd->ths', self.Q_, self.K_) / np.sqrt(self.head_dim)    # Einsum : used to write operations using indices ;  doing dot product here for query and key vectors for all heads\n",
        "        self.weights = softmax(self.scores, axis=2)  # shape of scores : (T,num_heads,T)attention from every token to other token\n",
        "        #For each token t and head h, compute attention distribution over all tokens s (including itself).\n",
        "        self.attn_output = np.einsum('ths,shd->thd', self.weights, self.V_)\n",
        "        self.concat_output = self.attn_output.reshape(T, D)\n",
        "        self.output = self.concat_output @ self.W_o\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        T, D = self.x.shape\n",
        "\n",
        "        # dOutput wrt W_o and concat_output\n",
        "        self.dW_o += self.concat_output.T @ d_out\n",
        "        d_concat = d_out @ self.W_o.T  # (T, D)\n",
        "\n",
        "        # Back to per-head output\n",
        "        d_attn_output = d_concat.reshape(T, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Attention weights gradients\n",
        "        d_weights = np.einsum('thd,shd->ths', d_attn_output, self.V_)\n",
        "        dV_ = np.einsum('ths,thd->shd', self.weights, d_attn_output)  # corrected einsum\n",
        "\n",
        "        # Backprop through softmax\n",
        "        dscores = dsoftmax(self.weights, d_weights) / np.sqrt(self.head_dim)\n",
        "\n",
        "        dQ_ = np.einsum('ths,shd->thd', dscores, self.K_)\n",
        "        dK_ = np.einsum('ths,thd->shd', dscores, self.Q_)\n",
        "\n",
        "        # Reshape back to (T, D)\n",
        "        dQ = dQ_.reshape(T, D)\n",
        "        dK = dK_.reshape(T, D)\n",
        "        dV = dV_.reshape(T, D)\n",
        "\n",
        "        self.dW_q += self.x.T @ dQ\n",
        "        self.dW_k += self.x.T @ dK\n",
        "        self.dW_v += self.x.T @ dV\n",
        "\n",
        "        dx_q = dQ @ self.W_q.T\n",
        "        dx_k = dK @ self.W_k.T\n",
        "        dx_v = dV @ self.W_v.T\n",
        "\n",
        "        dx = dx_q + dx_k + dx_v\n",
        "        return dx\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.dW_q.fill(0)\n",
        "        self.dW_k.fill(0)\n",
        "        self.dW_v.fill(0)\n",
        "        self.dW_o.fill(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rrl58YhmIzdQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}