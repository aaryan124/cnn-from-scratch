{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BatchNorm1D"
      ],
      "metadata": {
        "id": "a_gu25k3Ayc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Nom standardizes x in simple way like Standard Scalar but it also changes\n",
        "y as y =γ.x + β.\n",
        "\n",
        "---\n",
        "\n",
        "*   γ = scale default = 1\n",
        "*   β = shift default = 0"
      ],
      "metadata": {
        "id": "UWn5VUfbE4OU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we are using axis =0 because all operations here are done along batch dimension"
      ],
      "metadata": {
        "id": "j34X-lzcGfYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Example : x.shape = (32,8) # batch of 32 samples,each with 8 features\n",
        " then mu.shape = (1,8) Mean per feature"
      ],
      "metadata": {
        "id": "sKf0qezCH7lI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oG8-uk5AAgx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class BatchNorm1D:\n",
        "  def __init__(self,dim ,eps = 1e-5 ,momentum =0.9):\n",
        "    self.dim = dim\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.gamma = np.ones((1,dim)) # default initialization\n",
        "    self.beta = np.zeros((1,dim))\n",
        "    self.running_mean = np.zeros((1,dim))\n",
        "    self.running_var = np.ones((1,dim))\n",
        "    self.dgamma = np.zeros_like(self.gamma)\n",
        "    self.dbeta = np.zeros_like(self.beta)\n",
        "\n",
        "  def forward(self,x,training = True):\n",
        "    self.x = x\n",
        "    if training:\n",
        "      self.mu = np.mean(x,axis=0,keepdims = True)\n",
        "      self.var = np.var(x,axis=0,keepdims = True)\n",
        "      self.std = np.sqrt(self.var + self.eps)\n",
        "      self.x_hat = (x - self.mu)/self.std\n",
        "      # exponential moving average\n",
        "      self.running_mean = self.momentum * self.running_mean + (1-self.momentum)*self.mu\n",
        "      self.running_var = self.momentum * self.running_var + (1-self.momentum)*self.var\n",
        "      #An Exponential Moving Average (EMA) is a type of moving average\n",
        "      #that gives more weight to recent prices, making it more responsive to price changes\n",
        "      #than a Simple Moving Average (SMA)\n",
        "    else :\n",
        "      self.x_hat = (x -self.running_mean)/np.sqrt(self.running_var + self.eps)\n",
        "    # normalising might distort some useful signal so we restore some flexibility\n",
        "    out = self.gamma * self.x_hat + self.beta\n",
        "    return out\n",
        "\n",
        "  def backward(self,dout):\n",
        "    N,D = self.x.shape\n",
        "    self.dbeta = np.sum(dout,axis=0,keepdims =True) # since do_db = 1 gradient accumulate over batch\n",
        "    self.dgamma =np.sum(dout*self.x_hat,axis=0,keepdims=True)  # Acuumulate over batch\n",
        "    dx_hat = dout * self.gamma\n",
        "    dvar = np.sum(dx_hat *(self.x - self.mu)* -0.5*(self.std**-3),axis=0,keepdims=True)\n",
        "    dmu = np.sum(dx_hat*-1/self.std,axis=0,keepdims=True)+dvar*np.mean(-2*(self.x-self.mu),axis =0,keepdims=True)\n",
        "    dx = dx_hat * -1/self.std + dvar *2*(self.x - self.mu)/N+dmu/N\n",
        "    return dx\n",
        "\n",
        "  def zero_grad(self):\n",
        "    self.dgamma.fill(0)\n",
        "    self.dbeta.fill(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BatchNorm2D"
      ],
      "metadata": {
        "id": "qwc-DyMjOrvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In BatchNorm2D normalization is done per channel resulting in mu and var of shape (1,C,1,1).\n",
        "\n",
        "---\n",
        " Because convolution operates channel-wise\n",
        "Each filter (kernel) slides over a single channel at a time (shared weights across spatial dimensions).\n",
        "\n",
        "So it's logical to compute mean and variance per channel, not per pixel.\n",
        "\n"
      ],
      "metadata": {
        "id": "D_gNcOldIiEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class BatchNorm2D:\n",
        "    def __init__(self, num_channels, eps=1e-5, momentum=0.9):\n",
        "        self.num_channels = num_channels\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        self.gamma = np.ones((1, num_channels, 1, 1))  # scale\n",
        "        self.beta = np.zeros((1, num_channels, 1, 1))  # shift\n",
        "        self.running_mean = np.zeros((1, num_channels, 1, 1))\n",
        "        self.running_var = np.ones((1, num_channels, 1, 1))\n",
        "        self.dgamma = np.zeros_like(self.gamma)\n",
        "        self.dbeta = np.zeros_like(self.beta)\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        self.x = x\n",
        "        N, C, H, W = x.shape\n",
        "\n",
        "        if training:\n",
        "            # Compute mean and variance over (N, H, W)\n",
        "            self.mu = np.mean(x, axis=(0, 2, 3), keepdims=True)\n",
        "            self.var = np.var(x, axis=(0, 2, 3), keepdims=True)\n",
        "            self.std = np.sqrt(self.var + self.eps)\n",
        "            self.x_hat = (x - self.mu) / self.std\n",
        "            # Update running statistics\n",
        "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mu\n",
        "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
        "        else:\n",
        "            self.x_hat = (x - self.running_mean) / np.sqrt(self.running_var + self.eps)\n",
        "\n",
        "        out = self.gamma * self.x_hat + self.beta\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, C, H, W = dout.shape\n",
        "        m = N * H * W\n",
        "        # Gradients w.r.t. scale and shift\n",
        "        self.dbeta = np.sum(dout, axis=(0, 2, 3), keepdims=True)\n",
        "        self.dgamma = np.sum(dout * self.x_hat, axis=(0, 2, 3), keepdims=True)\n",
        "        dx_hat = dout * self.gamma\n",
        "        dvar = np.sum(dx_hat * (self.x - self.mu) * -0.5 * (self.std ** -3), axis=(0, 2, 3), keepdims=True)\n",
        "        dmu = np.sum(dx_hat * -1 / self.std, axis=(0, 2, 3), keepdims=True) + dvar * np.mean(-2 * (self.x - self.mu), axis=(0, 2, 3), keepdims=True)\n",
        "        dx = dx_hat / self.std + dvar * 2 * (self.x - self.mu) / m + dmu / m\n",
        "        return dx\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.dgamma.fill(0)\n",
        "        self.dbeta.fill(0)\n"
      ],
      "metadata": {
        "id": "gmCZm8HTEklm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NfvexLoNP1Ks"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}