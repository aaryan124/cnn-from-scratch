{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# VIT Implementation"
      ],
      "metadata": {
        "id": "xVds0U_Dv2c3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(B, N, 3, h, d)  ==  (2, 8, 3, 4, 16)\n",
        "This means:\n",
        "\n",
        "2 batches\n",
        "\n",
        "8 tokens\n",
        "\n",
        "3 values (q, k, v)\n",
        "\n",
        "4 attention heads\n",
        "\n",
        "16-dimensional space per head\n"
      ],
      "metadata": {
        "id": "uDblssWUrYdE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-nb_kzVofpvA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PatchEmbedding(nn.Module): # Turning image into tokens\n",
        "\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=64):\n",
        "        super().__init__()\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)  # Splits image into non-overlapping patches of size patch_size × patch_size\n",
        "                                                                                                  # It mimics splitting image into tokens, like words\n",
        "                                                                                                  # Each image patch is converted into a vector of length embed_dim.\n",
        "                                                                                                  # for example : The kernel shape is (64, 3, 4, 4),It slides a 4×4 patch over the image\n",
        "                                                                                                  # and for each patch, it applies all 64 filters.Each filter reduces that 3×4×4 patch → a single number ,64 values per patch\n",
        "    def forward(self, x):\n",
        "        # x: (B, 3, 32, 32)\n",
        "        x = self.proj(x)  # (B, embed_dim, H/patch, W/patch)\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)                           # (B,64,8,8) -> (B,64,8*8) -> transpose(1,2) -> This swaps dimensions dim1 and dim2 of a tensor.\n",
        "                                                                                                  # we went from filters,image per filter to pixel,representation from all filters in a vector (Simplified idea)\n",
        "        return x\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        assert dim % num_heads == 0\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3) # Single layer computes Q,K,V\n",
        "        self.out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape\n",
        "        qkv = self.qkv(x)  # (B, N, 3D)\n",
        "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)          # (B,N,3,H,D) -> (3,B,H,N,D).permute reorders the dimensions of a tensor\n",
        "                                                                                                  #You're not changing the data, just relabeling how it’s interpreted — like rotating or transposing dimensions\n",
        "\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        scores = (q @ k.transpose(-2, -1)) / self.head_dim**0.5  # (B, heads, N, N)\n",
        "        attn = scores.softmax(dim=-1)\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B, N, D)\n",
        "        return self.out(out)\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiHeadSelfAttention(dim, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(int(dim * mlp_ratio), dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, num_classes=10,\n",
        "                 embed_dim=64, depth=6, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim) #(B, N, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # (batch,num of tokens,dim_same_as_patch) prepending makes (B, N+1, embed_dim)\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + num_patches, embed_dim))\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerEncoderBlock(embed_dim, num_heads) for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)  # (B, N, D)\n",
        "        cls = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls, x), dim=1)\n",
        "        x = x + self.pos_embed[:, :x.size(1), :]\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])  # Class token\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zhURwvR-3Lb_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}