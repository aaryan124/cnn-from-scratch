{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RNN implementation"
      ],
      "metadata": {
        "id": "PhNHtRNWFaIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class RNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size, seq_len):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # Initialize weights\n",
        "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01\n",
        "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "        self.Why = np.random.randn(output_size, hidden_size) * 0.01\n",
        "        self.bh = np.zeros((hidden_size, 1))\n",
        "        self.by = np.zeros((output_size, 1))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.hs = {} #Initialize a dictionary hs to store the hidden state at each time step.We need it in backward pass\n",
        "        self.hs[-1] = np.zeros((self.hidden_size, 1))  # h₋₁ = 0\n",
        "\n",
        "        for t in range(self.seq_len): # t is time step\n",
        "            x_t = inputs[t].reshape(-1, 1) #Extract the input at time t and make it a column vector.\n",
        "            self.hs[t] = np.tanh(self.Wxh @ x_t + self.Whh @ self.hs[t - 1] + self.bh) #use previous hidden state as well\n",
        "            # store hs at each timestep\n",
        "        output = self.Why @ self.hs[self.seq_len - 1] + self.by # use last time step hidden state\n",
        "        return output\n",
        "\n",
        "    def backward(self, dL_dy, lr=0.01):\n",
        "        dWhy = dL_dy @ self.hs[self.seq_len - 1].T\n",
        "        dby = dL_dy\n",
        "\n",
        "        dWxh = np.zeros_like(self.Wxh)\n",
        "        dWhh = np.zeros_like(self.Whh)\n",
        "        dbh = np.zeros_like(self.bh)\n",
        "        dh_next = np.zeros((self.hidden_size, 1))\n",
        "\n",
        "        for t in reversed(range(self.seq_len)):\n",
        "            dh = self.Why.T @ dL_dy if t == self.seq_len - 1 else dh_next\n",
        "            dh_raw = (1 - self.hs[t] ** 2) * dh  # tanh'\n",
        "            dbh += dh_raw\n",
        "            dWxh += dh_raw @ self.inputs[t].reshape(1, -1)\n",
        "            dWhh += dh_raw @ self.hs[t - 1].T\n",
        "            dh_next = self.Whh.T @ dh_raw\n",
        "\n",
        "        # Gradient step\n",
        "        self.Wxh -= lr * dWxh\n",
        "        self.Whh -= lr * dWhh\n",
        "        self.Why -= lr * dWhy\n",
        "        self.bh -= lr * dbh\n",
        "        self.by -= lr * dby\n"
      ],
      "metadata": {
        "id": "q5tepSy9a2tX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}