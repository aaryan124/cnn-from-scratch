{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iFclDrXqKCyu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "# --- Activations and Loss ---\n",
        "def relu(x: np.ndarray) -> np.ndarray:\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_deriv(x: np.ndarray) -> np.ndarray:\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x: np.ndarray) -> np.ndarray:\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / np.sum(e_x)\n",
        "\n",
        "# single data point\n",
        "def cross_entropy_loss(pred: np.ndarray, label: int) -> float:\n",
        "    return float(-np.log(pred[label].item() + 1e-7))\n",
        "\n",
        "def cross_entropy_deriv(pred: np.ndarray, label: int) -> np.ndarray:\n",
        "    grad = pred.copy()\n",
        "    grad[label] -= 1 # since only 1 class so true label =1 grad = pred - y_true\n",
        "    return grad\n",
        "\n",
        "def flatten(x: np.ndarray) -> np.ndarray:\n",
        "    return x.reshape(-1, 1)\n",
        "\n",
        "def unflatten(x: np.ndarray, shape: Tuple[int, int, int]) -> np.ndarray:\n",
        "    return x.reshape(shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "class Conv2D:\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int,\n",
        "        stride: int = 1,\n",
        "        padding: int = 0\n",
        "    ):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        # Xavier/He init\n",
        "        scale = np.sqrt(2. / (in_channels * kernel_size * kernel_size))\n",
        "        # Each filter needs to combine information from all in_channels. So for every output feature map (each out_channel), there is a stack of filters, one per input channel.\n",
        "        self.filters = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * scale          # normal distribution mean =0; std =1 ,scale only changes std\n",
        "        # suppose y = Wx, then W (m,n) x(n,1) then var(y) = n * var(W) *var(x) ,to keep variance from not exploding var(W) =1/n\n",
        "        self.biases = np.zeros((out_channels, 1)) # out_channel or filter ,bias is one scalar per filter\n",
        "        # we need to define these here as they are learnable parameters .\n",
        "\n",
        "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
        "        self.last_input = input  # shape: (in_channels, H, W)  During the forward pass, we compute the output from the input.\n",
        "\n",
        "        C, H, W = input.shape\n",
        "        K = self.kernel_size\n",
        "        S = self.stride\n",
        "        P = self.padding\n",
        "\n",
        "        # Apply padding\n",
        "        if P > 0:\n",
        "            padded_input = np.pad(input, ((0, 0), (P, P), (P, P)), mode='constant')  # no pad on channel axis ,(top,bottom),(left,right) by P\n",
        "        else:\n",
        "            padded_input = input\n",
        "\n",
        "        self.padded_input = padded_input\n",
        "\n",
        "        H_out = (H + 2 * P - K) // S + 1\n",
        "        W_out = (W + 2 * P - K) // S + 1\n",
        "\n",
        "        output = np.zeros((self.out_channels, H_out, W_out)) # output tensor from conv layer\n",
        "\n",
        "        for oc in range(self.out_channels):\n",
        "            for i in range(H_out):\n",
        "                for j in range(W_out):\n",
        "                    region = padded_input[:, i*S:i*S+K, j*S:j*S+K]   # all input channels;extracting a region of shape (in_channels, K, K);(in channels ,verticle slice,horizontal slice)\n",
        "                    output[oc, i, j] = np.sum(region * self.filters[oc]) + self.biases[oc].item() # element wise mult\n",
        "        self.last_output = output\n",
        "        return output\n",
        "\n",
        "    def backward(self, dL_dout: np.ndarray, lr: float) -> np.ndarray:\n",
        "        C, H, W = self.last_input.shape\n",
        "        K = self.kernel_size\n",
        "        S = self.stride\n",
        "        P = self.padding\n",
        "        H_out, W_out = dL_dout.shape[1:] #It’s the gradient of the loss with respect to the output of the convolution layer.Shape: (out_channels, H_out, W_out)\n",
        "        #It slices the shape starting from index 1: shape[0] → out_channels ,shape[1] → H_out ,shape[2] → W_out\n",
        "\n",
        "        dL_dfilters = np.zeros_like(self.filters)\n",
        "        dL_dbiases = np.zeros_like(self.biases)\n",
        "        dL_dinput_padded = np.zeros_like(self.padded_input)  # same shape\n",
        "\n",
        "        # Compute gradients\n",
        "        for oc in range(self.out_channels):\n",
        "            for i in range(H_out):\n",
        "                for j in range(W_out):\n",
        "                    region = self.padded_input[:, i*S:i*S+K, j*S:j*S+K]\n",
        "                    dL_dfilters[oc] += dL_dout[oc, i, j] * region      #The loss gradient at this output pixel (dL_dout[oc, i, j]) gets multiplied by the input patch,This is the gradient of the filter weights — how much this filter should be adjusted\n",
        "                    dL_dbiases[oc] += dL_dout[oc, i, j]\n",
        "                    dL_dinput_padded[:, i*S:i*S+K, j*S:j*S+K] += dL_dout[oc, i, j] * self.filters[oc] #you're scaling the filter by how much error the output had.; scalar*(in,K,K)\n",
        "                    # dL_df=dL_do *do_df =dL_do * region as filter is constant\n",
        "        # Gradient descent update\n",
        "        self.filters -= lr * dL_dfilters\n",
        "        self.biases -= lr * dL_dbiases\n",
        "\n",
        "        # Remove padding from gradient if it was added\n",
        "        if self.padding > 0:\n",
        "            return dL_dinput_padded[:, self.padding:-self.padding, self.padding:-self.padding]\n",
        "        else:\n",
        "            return dL_dinput_padded\n",
        "        #dL_dinput_padded[:, P:-P, P:-P] ; it removes P pixels from both sides .\n"
      ],
      "metadata": {
        "id": "MhtQh7e3c5_3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool2x2:\n",
        "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
        "        self.last_input = input\n",
        "        n_filters, h, w = input.shape\n",
        "        out = np.zeros((n_filters, h // 2, w // 2))\n",
        "        self.max_pos = np.zeros_like(input)\n",
        "\n",
        "        for f in range(n_filters):\n",
        "            for i in range(0, h, 2):  # Loop over rows, stepping by 2\n",
        "                for j in range(0, w, 2):# Loop over columns, stepping by 2\n",
        "                    region = input[f, i:i+2, j:j+2]\n",
        "                    max_val = np.max(region)\n",
        "                    out[f, i//2, j//2] = max_val\n",
        "                    for ii in range(2):\n",
        "                        for jj in range(2):\n",
        "                            if region[ii, jj] == max_val: #region[ii, jj] iterates over the 2×2 region.\n",
        "                                self.max_pos[f, i+ii, j+jj] = 1\n",
        "        return out\n",
        "\n",
        "    def backward(self, dL_dout: np.ndarray) -> np.ndarray:\n",
        "        dL_dinput = np.zeros_like(self.last_input) # the gradient of the loss with respect to the input of the max pooling layer\n",
        "        for f in range(dL_dout.shape[0]):\n",
        "            for i in range(dL_dout.shape[1]):\n",
        "                for j in range(dL_dout.shape[2]):\n",
        "                    region = self.max_pos[f, i*2:i*2+2, j*2:j*2+2]\n",
        "                    dL_dinput[f, i*2:i*2+2, j*2:j*2+2] += region * dL_dout[f, i, j] #Only the max position in region is 1, rest are 0, so the scalar gets passed to the correct place.\n",
        "\n",
        "        return dL_dinput\n"
      ],
      "metadata": {
        "id": "UutgRr7a4bgH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense:\n",
        "    def __init__(self, input_size: int, output_size: int):\n",
        "        self.weights = np.random.randn(output_size, input_size) * 0.1\n",
        "        self.bias = np.zeros((output_size, 1))\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.last_input = x\n",
        "        return np.dot(self.weights, x) + self.bias\n",
        "\n",
        "    def backward(self, dL_dout: np.ndarray, lr: float) -> np.ndarray:\n",
        "        dL_dw = np.dot(dL_dout, self.last_input.T)\n",
        "        dL_db = dL_dout\n",
        "        dL_dinput = np.dot(self.weights.T, dL_dout)\n",
        "\n",
        "        self.weights -= lr * dL_dw\n",
        "        self.bias -= lr * dL_db\n",
        "        return dL_dinput\n"
      ],
      "metadata": {
        "id": "9D8Muwj6_J5y"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "\n",
        "# Normalize to [0,1]\n",
        "train_X = train_X / 255.0\n",
        "test_X = test_X / 255.0\n",
        "\n",
        "# Reshape to (num_samples, 1, 28, 28)\n",
        "train_X = train_X.reshape(-1, 1, 28, 28)\n",
        "test_X = test_X.reshape(-1, 1, 28, 28)\n",
        "\n",
        "# Optionally one-hot encode labels if using that\n"
      ],
      "metadata": {
        "id": "cpu21jgACAVI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN:\n",
        "    def __init__(self):\n",
        "        # Conv2D: 1 input channel (grayscale), 1 filter, kernel 3x3, stride=1, padding=0\n",
        "        self.conv = Conv2D(\n",
        "            in_channels=1,\n",
        "            out_channels=1,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=0\n",
        "        )\n",
        "\n",
        "        # After Conv2D: 28x28 → 26x26 (kernel=3, stride=1, padding=0)\n",
        "        # After MaxPool2x2: 26x26 → 13x13\n",
        "        self.pool = MaxPool2x2()\n",
        "\n",
        "        # Flattened size: 1 channel × 13 × 13 = 169\n",
        "        self.fc = Dense(input_size=169, output_size=10)\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        x = self.conv.forward(x)\n",
        "        self.relu_out = relu(x)\n",
        "        x = self.pool.forward(self.relu_out)\n",
        "        self.shape_before_flat = x.shape\n",
        "        x = flatten(x)\n",
        "        self.logits = self.fc.forward(x)\n",
        "        return softmax(self.logits)\n",
        "\n",
        "    def train_step(self, x: np.ndarray, y_true: int, lr: float = 0.01) -> Tuple[float, np.ndarray]:\n",
        "        out = self.forward(x)\n",
        "        loss = cross_entropy_loss(out, y_true)\n",
        "        grad = cross_entropy_deriv(out, y_true).reshape(-1, 1)\n",
        "\n",
        "        # Backpropagation\n",
        "        d_fc = self.fc.backward(grad, lr)\n",
        "        d_flat = unflatten(d_fc, self.shape_before_flat)\n",
        "        d_pool = self.pool.backward(d_flat)\n",
        "        d_relu = d_pool * relu_deriv(self.relu_out)\n",
        "        self.conv.backward(d_relu, lr)\n",
        "\n",
        "        return loss, out\n",
        "#During backpropagation, the gradient at each layer has the same shape as that layer's input — because we’re computing how the loss changes w.r.t. each input pixel of that layer."
      ],
      "metadata": {
        "id": "x0hnwvySCioR"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleCNN()\n",
        "epochs = 1\n",
        "lr = 0.01\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss_sum = 0\n",
        "    correct = 0\n",
        "    for i in range(len(train_X)):\n",
        "        x = train_X[i]\n",
        "        y = train_y[i]\n",
        "\n",
        "        loss, out = model.train_step(x, y, lr)\n",
        "        loss_sum += loss\n",
        "        pred = np.argmax(out)\n",
        "        correct += (pred == y)\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print(f\"Step {i}, Loss: {loss:.4f}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Avg Loss = {loss_sum / len(train_X):.4f}, Accuracy = {correct / len(train_X):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIoYcWWHCtBu",
        "outputId": "aa5df305-da8e-45e0-c33d-849f6c9ab09c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, Loss: 1.8340\n",
            "Step 1000, Loss: 0.0241\n",
            "Step 2000, Loss: 1.9090\n",
            "Step 3000, Loss: 1.7064\n",
            "Step 4000, Loss: 0.0604\n",
            "Step 5000, Loss: 0.0772\n",
            "Step 6000, Loss: 0.0001\n",
            "Step 7000, Loss: 0.5587\n",
            "Step 8000, Loss: 0.0015\n",
            "Step 9000, Loss: 0.1391\n",
            "Step 10000, Loss: 0.0001\n",
            "Step 11000, Loss: 0.0111\n",
            "Step 12000, Loss: 0.0310\n",
            "Step 13000, Loss: 0.4963\n",
            "Step 14000, Loss: 0.0243\n",
            "Step 15000, Loss: 0.0666\n",
            "Step 16000, Loss: 0.0136\n",
            "Step 17000, Loss: 0.0152\n",
            "Step 18000, Loss: 0.2762\n",
            "Step 19000, Loss: 0.0117\n",
            "Step 20000, Loss: 0.0002\n",
            "Step 21000, Loss: 0.7302\n",
            "Step 22000, Loss: 0.1260\n",
            "Step 23000, Loss: 0.0000\n",
            "Step 24000, Loss: 0.0724\n",
            "Step 25000, Loss: 0.4257\n",
            "Step 26000, Loss: 0.0065\n",
            "Step 27000, Loss: 0.5249\n",
            "Step 28000, Loss: 0.0034\n",
            "Step 29000, Loss: 0.0191\n",
            "Step 30000, Loss: 0.0020\n",
            "Step 31000, Loss: 1.8763\n",
            "Step 32000, Loss: 0.0147\n",
            "Step 33000, Loss: 1.0887\n",
            "Step 34000, Loss: 0.0297\n",
            "Step 35000, Loss: 0.0103\n",
            "Step 36000, Loss: 0.1104\n",
            "Step 37000, Loss: 0.0014\n",
            "Step 38000, Loss: 0.0996\n",
            "Step 39000, Loss: 0.7441\n",
            "Step 40000, Loss: 0.0014\n",
            "Step 41000, Loss: 0.0338\n",
            "Step 42000, Loss: 0.0025\n",
            "Step 43000, Loss: 0.9433\n",
            "Step 44000, Loss: 0.0120\n",
            "Step 45000, Loss: 0.0531\n",
            "Step 46000, Loss: 0.0590\n",
            "Step 47000, Loss: 0.0008\n",
            "Step 48000, Loss: 0.0618\n",
            "Step 49000, Loss: 0.0076\n",
            "Step 50000, Loss: 0.0602\n",
            "Step 51000, Loss: 0.0248\n",
            "Step 52000, Loss: 0.0697\n",
            "Step 53000, Loss: 0.0873\n",
            "Step 54000, Loss: 0.9639\n",
            "Step 55000, Loss: 0.0326\n",
            "Step 56000, Loss: 0.0012\n",
            "Step 57000, Loss: 0.0002\n",
            "Step 58000, Loss: 0.0121\n",
            "Step 59000, Loss: 0.0051\n",
            "Epoch 1: Avg Loss = 0.3773, Accuracy = 0.8886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6zYW8KSqGPaq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}